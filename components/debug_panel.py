#!/usr/bin/env python3
"""
Painel de Debug Avan√ßado
Sistema completo de debugging baseado no 8505-viewer
"""

import streamlit as st
import json
from datetime import datetime
from typing import Dict, List, Optional
import traceback

def render_advanced_debug_panel(debug_logs: List[Dict], test_results: Dict):
    """
    Renderiza painel de debug avan√ßado
    
    Args:
        debug_logs: Lista de logs do sistema
        test_results: Resultados de testes executados
    """
    
    st.header("üîß Debug & Monitoramento Avan√ßado")
    
    # Estat√≠sticas gerais
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        total_logs = len(debug_logs)
        st.metric("Total de Logs", total_logs)
    
    with col2:
        error_count = sum(1 for log in debug_logs[-100:] if log["level"].upper() == "ERROR")
        st.metric("Erros Recentes", error_count, delta="√∫ltimas 100 opera√ß√µes")
    
    with col3:
        warning_count = sum(1 for log in debug_logs[-100:] if log["level"].upper() == "WARNING")
        st.metric("Avisos Recentes", warning_count)
    
    with col4:
        success_rate = 0
        if test_results:
            successful = sum(1 for r in test_results.values() if r.get('success', False))
            success_rate = successful / len(test_results) * 100
        st.metric("Taxa de Sucesso", f"{success_rate:.1f}%")
    
    # Tabs do debug
    debug_tab1, debug_tab2, debug_tab3 = st.tabs([
        "üìù Logs Estruturados", 
        "‚ö†Ô∏è An√°lise de Erros", 
        "üìä Performance"
    ])
    
    # Tab 1: Logs estruturados
    with debug_tab1:
        render_structured_logs(debug_logs)
    
    # Tab 2: An√°lise de erros
    with debug_tab2:
        render_error_analysis(debug_logs)
    
    # Tab 3: Performance
    with debug_tab3:
        render_performance_analysis(test_results)

def render_structured_logs(debug_logs: List[Dict]):
    """Renderiza logs de forma estruturada"""
    
    st.subheader("üìù Logs Estruturados")
    
    # Filtros
    col1, col2, col3 = st.columns(3)
    
    with col1:
        max_logs = st.number_input("M√°ximo de logs:", value=50, min_value=10, max_value=200)
    
    with col2:
        level_filter = st.selectbox("Filtrar por n√≠vel:", ["all", "error", "warning", "info"])
    
    with col3:
        search_term = st.text_input("Buscar nos logs:", placeholder="Digite termo...")
    
    # Aplicar filtros
    filtered_logs = debug_logs[-max_logs:]
    
    if level_filter != "all":
        filtered_logs = [log for log in filtered_logs if log["level"].lower() == level_filter]
    
    if search_term:
        filtered_logs = [log for log in filtered_logs 
                        if search_term.lower() in log["message"].lower()]
    
    # Exibir logs em grid
    if filtered_logs:
        # Mostrar em grid de 2 colunas
        for i in range(0, len(filtered_logs), 2):
            cols = st.columns(2)
            
            for j, col in enumerate(cols):
                if i + j < len(filtered_logs):
                    log = filtered_logs[i + j]
                    render_log_card(log, col)
    else:
        st.info("üìã Nenhum log corresponde aos filtros aplicados")

def render_log_card(log: Dict, container):
    """Renderiza card individual de log"""
    
    timestamp = log["timestamp"].split("T")[1][:8]
    level = log["level"].upper()
    message = log["message"]
    
    # Estilo baseado no n√≠vel
    level_styles = {
        "ERROR": {"color": "#fee", "border": "#dc3545", "emoji": "üî¥"},
        "WARNING": {"color": "#fff8e1", "border": "#ffc107", "emoji": "üü°"},
        "INFO": {"color": "#e8f5e8", "border": "#28a745", "emoji": "üîµ"}
    }
    
    style = level_styles.get(level, level_styles["INFO"])
    
    with container:
        st.markdown(f"""
        <div style="border: 2px solid {style['border']}; border-radius: 10px; 
                    padding: 15px; margin: 10px 0; background: {style['color']};
                    transition: transform 0.2s ease;">
            <div style="display: flex; align-items: center; margin-bottom: 10px;">
                <span style="font-size: 18px; margin-right: 10px;">{style['emoji']}</span>
                <strong style="color: #333;">[{timestamp}] {level}</strong>
            </div>
            <div style="color: #555; font-size: 14px; margin-bottom: 10px; line-height: 1.4;">
                {message}
            </div>
        </div>
        """, unsafe_allow_html=True)
        
        # Detalhes expandidos
        if log.get("details"):
            with st.expander(f"üîç Detalhes ({timestamp})"):
                st.json(log["details"])
        
        # Stack trace para erros
        if level == "ERROR" and log.get("stack_trace"):
            with st.expander(f"üìö Stack Trace ({timestamp})"):
                for line in log["stack_trace"]:
                    st.code(line.strip(), language="python")

def render_error_analysis(debug_logs: List[Dict]):
    """An√°lise espec√≠fica de erros"""
    
    st.subheader("‚ö†Ô∏è An√°lise de Erros")
    
    # Filtrar apenas erros
    error_logs = [log for log in debug_logs if log["level"].upper() == "ERROR"]
    
    if not error_logs:
        st.success("üéâ Nenhum erro registrado!")
        return
    
    # Estat√≠sticas de erros
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("Total de Erros", len(error_logs))
        
        # Erros por categoria
        error_types = {}
        for log in error_logs:
            error_msg = log["message"]
            # Categorizar erros b√°sicos
            if "connection" in error_msg.lower():
                category = "Conex√£o"
            elif "timeout" in error_msg.lower():
                category = "Timeout"
            elif "json" in error_msg.lower():
                category = "Parsing"
            elif "permission" in error_msg.lower():
                category = "Permiss√µes"
            else:
                category = "Outros"
            
            error_types[category] = error_types.get(category, 0) + 1
        
        if error_types:
            st.bar_chart(error_types)
    
    with col2:
        # Erros mais recentes
        st.markdown("#### üî¥ Erros Mais Recentes")
        
        for log in error_logs[-5:]:  # √öltimos 5 erros
            timestamp = log["timestamp"].split("T")[1][:8]
            st.markdown(f"""
            <div style="background: #f8d7da; border-left: 4px solid #dc3545; 
                        padding: 10px; margin: 5px 0; border-radius: 5px;">
                <strong>[{timestamp}]</strong> {log["message"][:80]}...
            </div>
            """, unsafe_allow_html=True)

def render_performance_analysis(test_results: Dict):
    """An√°lise de performance dos testes"""
    
    st.subheader("üìä An√°lise de Performance")
    
    if not test_results:
        st.info("üìä Execute alguns testes para ver an√°lise de performance")
        return
    
    # Preparar dados
    successful_tests = [r for r in test_results.values() if r.get('success', False)]
    failed_tests = [r for r in test_results.values() if not r.get('success', False)]
    
    # M√©tricas gerais
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("Testes Bem-sucedidos", len(successful_tests))
    
    with col2:
        st.metric("Testes Falhados", len(failed_tests))
    
    with col3:
        if test_results:
            avg_time = sum(r.get('execution_time', 0) for r in test_results.values()) / len(test_results)
            st.metric("Tempo M√©dio", f"{avg_time:.2f}s")
    
    # An√°lise por tipo de resumo
    if successful_tests:
        st.markdown("#### üìä Performance por Tipo")
        
        type_performance = {}
        for test in successful_tests:
            summary_type = test.get('summary_type', 'unknown')
            exec_time = test.get('execution_time', 0)
            
            if summary_type not in type_performance:
                type_performance[summary_type] = {
                    "times": [],
                    "costs": [],
                    "tokens": []
                }
            
            type_performance[summary_type]["times"].append(exec_time)
            
            # M√©tricas se dispon√≠veis
            metrics = test.get('result', {}).get('metrics', {})
            if metrics:
                type_performance[summary_type]["costs"].append(metrics.get('cost', 0))
                total_tokens = metrics.get('input_tokens', 0) + metrics.get('output_tokens', 0)
                type_performance[summary_type]["tokens"].append(total_tokens)
        
        # Exibir an√°lise
        for summary_type, data in type_performance.items():
            avg_time = sum(data["times"]) / len(data["times"])
            avg_cost = sum(data["costs"]) / len(data["costs"]) if data["costs"] else 0
            avg_tokens = sum(data["tokens"]) / len(data["tokens"]) if data["tokens"] else 0
            
            st.markdown(f"""
            <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; 
                        margin: 10px 0; border-left: 4px solid #667eea;">
                <h5 style="margin: 0 0 10px 0; color: #333;">üìù {summary_type.title()}</h5>
                <div style="display: grid; grid-template-columns: repeat(auto-fit, minmax(120px, 1fr)); gap: 10px;">
                    <div><strong>‚è±Ô∏è Tempo:</strong> {avg_time:.2f}s</div>
                    <div><strong>üí∞ Custo:</strong> ${avg_cost:.6f}</div>
                    <div><strong>üî¢ Tokens:</strong> {avg_tokens:.0f}</div>
                    <div><strong>üìä Testes:</strong> {len(data["times"])}</div>
                </div>
            </div>
            """, unsafe_allow_html=True)

def create_diagnostic_report() -> str:
    """Cria relat√≥rio de diagn√≥stico do sistema"""
    
    report_time = datetime.now().strftime("%d/%m/%Y %H:%M:%S")
    
    # Verifica√ß√µes b√°sicas do sistema
    checks = []
    
    # Verificar se viewer HTTP est√° rodando
    try:
        import requests
        response = requests.get("http://localhost:3041/api/sessions", timeout=3)
        if response.status_code == 200:
            sessions = response.json()
            checks.append(f"‚úÖ Viewer HTTP: {len(sessions)} sess√µes ativas")
        else:
            checks.append(f"‚ùå Viewer HTTP: Status {response.status_code}")
    except:
        checks.append("‚ùå Viewer HTTP: N√£o responsivo")
    
    # Verificar API principal
    try:
        response = requests.get("http://localhost:8990/health", timeout=3)
        if response.status_code == 200:
            checks.append("‚úÖ API Principal: Online")
        else:
            checks.append(f"‚ùå API Principal: Status {response.status_code}")
    except:
        checks.append("‚ùå API Principal: N√£o responsivo")
    
    # Verificar arquivos do sistema
    from pathlib import Path
    
    claude_projects = Path("/home/suthub/.claude/projects")
    if claude_projects.exists():
        session_files = sum(len(list(d.glob("*.jsonl"))) for d in claude_projects.iterdir() if d.is_dir())
        checks.append(f"‚úÖ Sistema de arquivos: {session_files} sess√µes encontradas")
    else:
        checks.append("‚ùå Sistema de arquivos: Diret√≥rio n√£o encontrado")
    
    # Verificar Claude SDK
    sdk_path = Path("/home/suthub/.claude/cc-sdk-chat/viewer-claude/backend/claude-sdk")
    if sdk_path.exists():
        checks.append("‚úÖ Claude SDK: Linkado corretamente")
    else:
        checks.append("‚ùå Claude SDK: Link n√£o encontrado")
    
    # Gerar relat√≥rio
    report = f"""
# üîß Relat√≥rio de Diagn√≥stico do Sistema

**Gerado em:** {report_time}

## üåê Status dos Servi√ßos

{chr(10).join(checks)}

## üìä URLs dos Servi√ßos

- **Viewer HTTP (produ√ß√£o):** http://localhost:3041
- **Streamlit Modernizado:** http://localhost:3042 (nova interface)
- **API Principal:** http://localhost:8990
- **Debug Original:** http://localhost:8506

## üéØ Funcionalidades Ativas

‚úÖ Visualiza√ß√£o de sess√µes .jsonl
‚úÖ Sistema de resumo inteligente  
‚úÖ Claude SDK integrado
‚úÖ URLs diretas para sess√µes
‚úÖ Interface de debug avan√ßada
‚úÖ Sistema de m√©tricas

## üí° Recomenda√ß√µes

- Monitorar logs de erro regularmente
- Verificar performance dos resumos
- Manter APIs principais online
- Backup das m√©tricas coletadas

---
*Relat√≥rio gerado automaticamente pelo Claude Session Viewer*
"""
    
    return report

def export_debug_data() -> Dict:
    """Exporta dados de debug para an√°lise"""
    
    return {
        "export_timestamp": datetime.now().isoformat(),
        "system_status": create_diagnostic_report(),
        "debug_logs": st.session_state.get("debug_logs", [])[-100:],  # √öltimos 100
        "test_results": st.session_state.get("test_results", {}),
        "session_state_keys": list(st.session_state.keys()),
        "performance_summary": {
            "total_logs": len(st.session_state.get("debug_logs", [])),
            "total_tests": len(st.session_state.get("test_results", {})),
            "debug_mode_active": st.session_state.get("debug_mode", False)
        }
    }